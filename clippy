#!/usr/bin/env python3

import argparse
import json
import os
import sys
import platform
import textwrap
import subprocess
from typing import Dict, List, Any, Optional, Tuple, Callable
# --- Dependency Check ---
try:
    import requests
except ImportError:
    print(
        "Error: The 'requests' library is required but not found.",
        "Please install it: python -m pip install requests",
        file=sys.stderr,
    )
    sys.exit(1)

# --- Constants ---
CONFIG_DIR = os.path.expanduser("~/.clippy")
CONFIG_FILE = os.path.join(CONFIG_DIR, "config.json")
DEFAULT_TIMEOUT = 60  # Default request timeout in seconds
CLIPPY_REPO_URL = "https://github.com/nedn/clippy" # Added repo URL

# ANSI Color Codes
# Use functions for clarity and to ensure reset
def color_text(text: str, color_code: str) -> str:
    """Applies ANSI color code to text, ensuring reset."""
    # Disable color if output is not a TTY (e.g., redirected to file)
    if not sys.stdout.isatty():
        return text
    # Disable color on Windows if colorama is not installed (conservative)
    # Basic ANSI support is better in newer Windows terminals, but this is safer
    if platform.system() == "Windows":
        # Could add a check for WT_SESSION or similar env vars if needed
        pass # Allow colors, modern terminals often support them
    return f"{color_code}{text}\033[0m"

RED = "\033[31m"
GREEN = "\033[32m"
YELLOW = "\033[33m" # Added for warnings
CYAN = "\033[36m"
BOLD = "\033[1m"
RESET = "\033[0m" # Keep for internal use if needed

def error_print(*args, **kwargs):
    """Prints arguments to stderr in red."""
    print(color_text("Error:", RED), *args, file=sys.stderr, **kwargs)

def warn_print(*args, **kwargs):
    """Prints arguments to stderr in yellow."""
    print(color_text("Warning:", YELLOW), *args, file=sys.stderr, **kwargs)

def success_print(*args, **kwargs):
    """Prints arguments to stdout in green."""
    print(color_text("Success:", GREEN), *args, **kwargs)

def info_print(*args, **kwargs):
    """Prints arguments to stdout (standard color)."""
    print(*args, **kwargs)


# --- Update Check ---

def run_git_command(command: List[str]) -> Tuple[Optional[str], Optional[str], int]:
    """Runs a git command and returns stdout, stderr, and return code."""
    try:
        process = subprocess.run(
            ["git"] + command,
            capture_output=True,
            text=True,
            encoding='utf-8',
            check=False, # Don't raise exception on non-zero exit
            cwd=os.path.dirname(__file__) # Run git in the script's directory
        )
        return process.stdout.strip(), process.stderr.strip(), process.returncode
    except FileNotFoundError:
        # Git command not found
        return None, "Git command not found. Please ensure git is installed and in your PATH.", -1
    except Exception as e:
        return None, f"Failed to run git command: {e}", -1

def check_for_updates():
    """Checks if the local clippy script is behind the remote main branch."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # 1. Check if inside a git repository
    stdout, stderr, retcode = run_git_command(["rev-parse", "--is-inside-work-tree"])
    if retcode != 0 or stdout != 'true':
        # warn_print("Not running inside a git repository. Skipping update check.")
        return # Silently skip if not in a git repo

    # 2. Check if remote 'origin' points to the correct repo
    stdout, stderr, retcode = run_git_command(["remote", "get-url", "origin"])
    if retcode != 0 or not stdout or CLIPPY_REPO_URL not in stdout:
        # warn_print(f"Git remote 'origin' does not point to {CLIPPY_REPO_URL}. Skipping update check.")
        return # Silently skip if remote is wrong

    # 3. Fetch latest changes from origin/main (quietly)
    info_print("Checking for script updates...")
    _, stderr, retcode = run_git_command(["fetch", "origin", "main", "--quiet"])
    if retcode != 0:
        warn_print(f"Failed to fetch updates from remote: {stderr}. Skipping update check.")
        return

    # 4. Get local and remote commit hashes
    local_hash, stderr, retcode_local = run_git_command(["rev-parse", "HEAD"])
    remote_hash, stderr, retcode_remote = run_git_command(["rev-parse", "origin/main"])

    if retcode_local != 0 or retcode_remote != 0:
        warn_print("Could not determine local or remote commit hash. Skipping update check.")
        return

    if local_hash == remote_hash:
        # info_print("Clippy script is up to date.") # Optional: notify if up-to-date
        return

    # 5. Check if local commit is an ancestor of the remote commit
    # If `git merge-base --is-ancestor local_hash remote_hash` returns 0, local is an ancestor (or same)
    _, _, retcode_ancestor = run_git_command(["merge-base", "--is-ancestor", local_hash, remote_hash])

    if retcode_ancestor == 0:
        # Local commit IS an ancestor of the remote commit, meaning local is behind.
        warn_print(f"Your clippy script is behind the main branch ({CLIPPY_REPO_URL}).")
        warn_print("Consider updating by running: git pull origin main")
    # elif retcode_ancestor == 1:
        # Local commit is NOT an ancestor (diverged or ahead). No warning needed in this case.
        # pass
    # else:
        # Error running merge-base
        # warn_print("Could not compare local and remote versions. Skipping update check.")
        # pass


# --- Provider Configuration ---

# Define types for provider functions
HeaderFactory = Callable[[str], Dict[str, str]]
PayloadFactory = Callable[[str, List[Dict[str, str]], Optional[int], float], Dict[str, Any]]
ResponseParser = Callable[[Dict[str, Any]], Optional[str]]

class ProviderConfig:
    """Holds configuration and logic for a specific API provider type."""
    def __init__(self, base_url: str, header_factory: HeaderFactory, payload_factory: PayloadFactory, response_parser: ResponseParser):
        self.base_url = base_url
        self.header_factory = header_factory
        self.payload_factory = payload_factory
        self.response_parser = response_parser

# --- Provider Specific Implementations ---

# OpenAI & Compatible (e.g., Gemini via OpenAI endpoint)
def _openai_headers(api_key: str) -> Dict[str, str]:
    return {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

def _openai_payload(model: str, messages: List[Dict[str, str]], max_tokens: Optional[int], temperature: float) -> Dict[str, Any]:
    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    return payload

def _openai_parser(response: Dict[str, Any]) -> Optional[str]:
    try:
        # Check for standard OpenAI error structure first
        if "error" in response and isinstance(response["error"], dict):
            err_msg = response["error"].get("message", "Unknown OpenAI API Error")
            raise ValueError(f"API Error: {err_msg}")
        elif "error" in response: # Sometimes it might be just a string
            raise ValueError(f"API Error: {response['error']}")

        choice = response.get("choices", [{}])[0]
        message = choice.get("message", {})
        content = message.get("content")
        if content is None:
            # Check for alternative structure (less common now)
            content = choice.get("text") # Older completion endpoint style?
        return content
    except (IndexError, KeyError, AttributeError, TypeError) as e:
        raise ValueError(f"Could not parse API response structure: {e}. Response: {response}") from e


# Anthropic
def _anthropic_headers(api_key: str) -> Dict[str, str]:
    return {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01", # Consider making configurable if needed later
        "content-type": "application/json",
    }

def _anthropic_payload(model: str, messages: List[Dict[str, str]], max_tokens: Optional[int], temperature: float) -> Dict[str, Any]:
    system_prompt = next((msg["content"] for msg in messages if msg["role"] == "system"), None)
    # Anthropic requires messages alternating user/assistant, starting with user.
    # We need to filter and potentially adapt the history if it doesn't fit.
    # For simplicity here, we just take the last user message if no system prompt,
    # or combine system and last user message. A more robust solution would handle
    # proper conversation history merging/truncation.
    user_messages = [msg for msg in messages if msg["role"] == "user" or msg["role"] == "assistant"]
    # Ensure it starts with 'user' if possible, remove leading 'assistant' if necessary
    while user_messages and user_messages[0]["role"] == "assistant":
        user_messages.pop(0)
    # Ensure alternation (simple approach: remove consecutive same roles)
    valid_messages = []
    last_role = None
    for msg in user_messages:
        if msg["role"] != last_role:
            valid_messages.append(msg)
            last_role = msg["role"]
        # If roles are the same, we could try merging content, but skipping is simpler for now
        # else: # Optional: merge consecutive messages of the same role
        #     if valid_messages: valid_messages[-1]["content"] += "\n" + msg["content"]

    if not valid_messages and system_prompt:
        # If only a system prompt exists, Anthropic needs a dummy user message
        warn_print("Only system prompt provided; adding empty user message for Anthropic.")
        valid_messages.append({"role": "user", "content": "..."}) # Or maybe use the system prompt as user prompt?
    elif not valid_messages:
        raise ValueError("No valid user messages found for Anthropic payload")


    payload = {
        "model": model,
        "max_tokens": max_tokens or 1024, # Anthropic requires max_tokens
        "messages": valid_messages,
        "temperature": temperature,
    }
    if system_prompt:
        payload["system"] = system_prompt
    return payload

def _anthropic_parser(response: Dict[str, Any]) -> Optional[str]:
    try:
        # Check for Anthropic specific error structure
        if response.get("type") == "error":
            err_msg = response.get("error", {}).get("message", "Unknown Anthropic Error")
            raise ValueError(f"Anthropic API Error: {err_msg}")

        # Extract content (Anthropic's structure is different)
        content_blocks = response.get("content", [])
        if not content_blocks:
            # Handle streaming or alternative formats if necessary in the future
            return None # Or raise error if content is expected
        # Combine text from potentially multiple content blocks
        full_text = "".join(block.get("text", "") for block in content_blocks if block.get("type") == "text")
        return full_text if full_text else None
    except (IndexError, KeyError, AttributeError, TypeError) as e:
        raise ValueError(f"Could not parse Anthropic response structure: {e}. Response: {response}") from e

# --- Provider Registry ---

PROVIDER_TYPES = {
    "openai": ProviderConfig(
        base_url="https://api.openai.com/v1/chat/completions",
        header_factory=_openai_headers,
        payload_factory=_openai_payload,
        response_parser=_openai_parser,
    ),
    "google": ProviderConfig(
        # Using the OpenAI compatible endpoint for Gemini
        # Let's stick to the OpenAI Compatibility endpoint provided in the original code for simplicity
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/chat/completions",
        header_factory=_openai_headers, # Assumes Bearer token works (API Key might need different header) - REQUIRES VERIFICATION if using direct Google API Key
        # If using a Google API Key directly (not OAuth), header might be {'x-goog-api-key': api_key}
        payload_factory=_openai_payload,
        response_parser=_openai_parser, # Assumes OpenAI structure
    ),
    "anthropic": ProviderConfig(
        base_url="https://api.anthropic.com/v1/messages",
        header_factory=_anthropic_headers,
        payload_factory=_anthropic_payload,
        response_parser=_anthropic_parser,
    ),
}

# Mapping from model name prefixes to provider type keys
MODEL_PREFIX_TO_PROVIDER = {
    "gpt-": "openai",
    "gemini-": "google", # Maps to the google provider config
    "claude-": "anthropic",
}
DEFAULT_PROVIDER = "openai" # Fallback provider type

def get_provider_type_for_model(model_name: str) -> str:
    """Determines the provider type key based on the model name prefix."""
    for prefix, provider_key in MODEL_PREFIX_TO_PROVIDER.items():
        if model_name.startswith(prefix):
            return provider_key
    warn_print(f"Unknown model prefix for '{model_name}'. Falling back to provider type: '{DEFAULT_PROVIDER}'.")
    return DEFAULT_PROVIDER

# --- Configuration ---

def load_config() -> Dict[str, Any]:
    """Loads configuration from the config file."""
    default_config = {"models": {}, "default_model": None}
    if not os.path.exists(CONFIG_FILE):
        return default_config
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            # Ensure essential keys exist with defaults
            config.setdefault("models", {})
            config.setdefault("default_model", None)
            return config
    except (FileNotFoundError, json.JSONDecodeError, OSError) as e:
        warn_print(f"Could not load config file ({CONFIG_FILE}): {e}. Using default configuration.")
        return default_config

def save_config(config: Dict[str, Any]) -> bool:
    """Saves configuration to the config file. Returns True on success."""
    try:
        os.makedirs(CONFIG_DIR, exist_ok=True)
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=4, ensure_ascii=False)
        return True
    except OSError as e:
        error_print(f"Could not save config file ({CONFIG_FILE}): {e}")
        return False

# --- API Client ---

class ApiClient:
    """Client for making AI API requests using the requests library."""
    def __init__(self, session: Optional[requests.Session] = None):
        self.session = session or requests.Session()
        # Consider adding default headers like User-Agent here if desired
        # self.session.headers.update({"User-Agent": "ClippyAIClient/1.0"})

    def make_request(self, url: str, headers: Dict[str, str], payload: Dict[str, Any], timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
        """Makes a POST request and handles potential errors."""
        try:
            response = self.session.post(url, headers=headers, json=payload, timeout=timeout)
            response.raise_for_status() # Raises HTTPError for 4xx/5xx
            # Check if response is empty or not JSON before parsing
            if not response.content:
                raise ValueError("API returned an empty response.")
            try:
                return response.json()
            except json.JSONDecodeError as json_err:
                raise ValueError(f"API returned non-JSON response (Status: {response.status_code}): {response.text[:1000]}") from json_err # Limit error text length

        except requests.exceptions.Timeout as e:
            raise TimeoutError(f"Request timed out after {timeout} seconds.") from e
        except requests.exceptions.HTTPError as e:
            # Try to get a more specific message from the response body if possible
            error_msg = f"HTTP Error: {e.response.status_code} {e.response.reason}"
            try:
                error_data = e.response.json()
                # Attempt to extract common error patterns (deferring detailed parsing to provider)
                details = error_data.get('error', {}).get('message') or \
                          error_data.get('detail') or \
                          str(error_data) # Fallback to string representation
                error_msg += f" - {details}"
            except json.JSONDecodeError:
                error_msg += f" - {e.response.text[:500]}" # Include raw text snippet
            raise ConnectionError(error_msg) from e # Use ConnectionError for HTTP issues
        except requests.exceptions.RequestException as e:
            # Catch other request issues like connection errors, DNS errors etc.
            raise ConnectionError(f"Network or request setup error: {e}") from e
        except Exception as e: # Catch unexpected errors during the request process
            raise RuntimeError(f"An unexpected error occurred during the API request: {e}") from e


# --- Core Logic ---

def get_default_system_prompt() -> str:
    """Returns the default system prompt, augmented with OS information."""
    try:
        os_info = f"OS: {platform.system()} {platform.release()} ({platform.machine()})"
    except Exception:
        os_info = "OS: (Could not determine)"

    # Use textwrap.dedent for cleaner multiline string definition
    return textwrap.dedent(f"""
        You are a helpful command-line assistant called Clippy.
        Provide concise, accurate, and well-formatted responses suitable for a terminal.

        Guidelines:
        - **Brevity:** Be brief and to the point. Avoid unnecessary conversation.
        - **Formatting:** Use simple Markdown (bold `**`, lists `-`/`*`/`1.`, code blocks ```).
        - **Readability:** Keep lines reasonably short for terminal display.
        - **Clarity:** Focus on answering the request directly.
        - **Environment:** You are running in a terminal environment. {os_info}.

        Aim for efficiency and clarity in your output.
    """).strip()

def ask_ai(prompt: str, model_name: str, api_key: str, provider_type: str, system_prompt: Optional[str]) -> Optional[str]:
    """Sends the prompt to the specified AI model and returns the response."""
    info_print(f"Querying model '{color_text(model_name, CYAN)}' ({provider_type})...")

    provider = PROVIDER_TYPES.get(provider_type)
    if not provider:
        error_print(f"Provider type '{provider_type}' is not configured in PROVIDER_TYPES.")
        return None

    client = ApiClient()

    messages: List[Dict[str, str]] = []
    effective_system_prompt = system_prompt or get_default_system_prompt()
    if effective_system_prompt: # Add system prompt if available
        messages.append({"role": "system", "content": effective_system_prompt.strip()})
    messages.append({"role": "user", "content": prompt.strip()})

    try:
        headers = provider.header_factory(api_key)
        payload = provider.payload_factory(model_name, messages, None, 0.7) # Add args for max_tokens, temp if needed
        response_data = client.make_request(provider.base_url, headers, payload)
        parsed_response = provider.response_parser(response_data)
        return parsed_response

    except (ValueError, ConnectionError, TimeoutError, RuntimeError) as e:
        # These are errors expected from ApiClient or provider functions
        error_print(f"API interaction failed: {e}")
        return None
    except Exception as e:
        # Catch any truly unexpected errors
        error_print(f"An unexpected error occurred during AI request: {e}")
        # Consider logging traceback here for debugging if needed
        # import traceback; traceback.print_exc()
        return None

# --- Output Formatting ---

def format_terminal_output(text: str) -> str:
    """Basic formatting for terminal output (bold, code blocks)."""
    if not sys.stdout.isatty(): # Skip formatting if not TTY
        return text

    lines = text.splitlines()
    formatted_lines = []
    in_code_block = False

    i = 0
    while i < len(lines):
        line = lines[i]
        stripped_line = line.strip()

        if stripped_line.startswith("```"):
            in_code_block = not in_code_block
            # Keep the backticks, maybe add subtle color if desired
            formatted_lines.append(line)
        elif in_code_block:
            formatted_lines.append(color_text(line, CYAN))
        else:
            # Basic **bold** handling (doesn't handle nested or complex cases)
            # This simple replace might mess up if '**' appears naturally
            # A regex approach could be slightly better but still limited without a parser
            import re
            # Replace pairs of **text** with bolded text
            formatted_line = re.sub(r'\*\*(.*?)\*\*', lambda m: color_text(m.group(1), BOLD), line)
            formatted_lines.append(formatted_line)
        i += 1

    # Ensure reset at the very end if the last line was colored
    # Although color_text should handle this per-call.
    return "\n".join(formatted_lines)


# --- Command Functions ---

def set_model_cmd(args: argparse.Namespace, config: Dict[str, Any]) -> bool:
    """Sets the AI model and API key. Returns True on success."""
    try:
        model_name, api_key = args.model_api.split(":", 1)
    except ValueError:
        error_print("Invalid format. Use <model_name>:<api_key>")
        return False

    model_name = model_name.strip()
    api_key = api_key.strip()

    if not model_name or not api_key:
        error_print("Both model name and API key are required.")
        return False

    provider_type = get_provider_type_for_model(model_name)
    if provider_type not in PROVIDER_TYPES:
        warn_print(f"Model '{model_name}' maps to unknown provider type '{provider_type}'. Configuration might be incomplete.")
        # Decide if you want to allow this or return False

    config['models'][model_name] = {'api_key': api_key, 'provider_type': provider_type}
    info_print(f"Model '{color_text(model_name, CYAN)}' (type: {provider_type}) configured.")

    num_models = len(config['models'])
    if args.default or num_models == 1:
        config['default_model'] = model_name
        info_print(f"Model '{color_text(model_name, CYAN)}' set as default.")

    return save_config(config)


def _assemble_prompt(prompt_args: List[str]) -> str:
    """Assembles the prompt from arguments and stdin."""
    stdin_content = ""
    if not sys.stdin.isatty(): # Check if data is piped
        stdin_content = sys.stdin.read().strip()

    # Combine arguments and stdin content
    # Prioritize args, append stdin if present
    full_prompt = " ".join(prompt_args).strip()
    if stdin_content:
        if full_prompt:
            full_prompt += "\n\n" + stdin_content # Add separator if both exist
        else:
            full_prompt = stdin_content

    return full_prompt.strip()


def ask_cmd(args: argparse.Namespace, config: Dict[str, Any]) -> bool:
    """Handles the 'ask' command. Returns True on success."""
    if not config['models']:
        error_print("No models configured. Use 'clippy set_model <model_name>:<api_key>' first.")
        return False

    model_name = args.model or config.get('default_model')
    if not model_name:
        error_print("No model specified and no default model set.")
        info_print("Specify a model with --model or run 'clippy set_default <model_name>'.")
        list_models_cmd(args, config) # Show available models
        return False

    model_config = config['models'].get(model_name)
    if not model_config:
        available = ', '.join(sorted(config['models'].keys()))
        error_print(f"Model '{model_name}' not found.")
        info_print(f"Available models: {available}")
        return False

    api_key = model_config.get('api_key')
    provider_type = model_config.get('provider_type') # Get stored provider type

    if not api_key:
        error_print(f"API key for model '{model_name}' is missing in configuration.")
        return False
    if not provider_type:
        # Attempt to determine provider type on the fly if missing (legacy config?)
        warn_print(f"Provider type for model '{model_name}' missing in config. Determining from prefix.")
        provider_type = get_provider_type_for_model(model_name)
        # Optionally save the determined type back to config here if desired
        # config['models'][model_name]['provider_type'] = provider_type
        # save_config(config)

    prompt = _assemble_prompt(args.prompt)
    if not prompt:
        error_print("Prompt cannot be empty. Provide text as arguments or pipe via stdin.")
        return False

    # Allow overriding system prompt via arg in the future?
    # system_prompt = args.system_prompt or get_default_system_prompt()
    system_prompt = get_default_system_prompt()

    ai_response = ask_ai(prompt, model_name, api_key, provider_type, system_prompt=system_prompt)

    if ai_response is not None:
        formatted_response = format_terminal_output(ai_response.strip())
        print("\n" + color_text("AI Response:", BOLD) + "\n")
        print(formatted_response)
        return True
    else:
        # Error message already printed by ask_ai or dependent functions
        return False


def list_models_cmd(args: argparse.Namespace, config: Dict[str, Any]) -> bool:
    """Lists configured models. Returns True."""
    models = config.get('models', {})
    default_model = config.get('default_model')

    if not models:
        info_print("No models configured. Use 'clippy set_model <model_name>:<api_key>' to add one.")
        return True

    info_print("\n" + color_text("Configured Models:", BOLD))
    for name in sorted(models.keys()):
        provider_type = get_provider_type_for_model(name)
        is_default = (name == default_model)
        prefix = color_text("* ", GREEN) if is_default else "  "
        info_print(f"{prefix}{color_text(name, CYAN)} ({provider_type})")

    if default_model:
        info_print(f"\n({color_text('*', GREEN)} indicates the default model)")
    else:
        info_print("\nNo default model set. Use 'clippy set_default <model_name>'.")
    return True


def set_default_cmd(args: argparse.Namespace, config: Dict[str, Any]) -> bool:
    """Changes the default model. Returns True on success."""
    model_name = args.model.strip()

    if not config.get('models'):
        error_print("No models configured yet.")
        return False

    if model_name not in config['models']:
        available = ', '.join(sorted(config['models'].keys()))
        error_print(f"Model '{model_name}' not found.")
        info_print(f"Available models: {available}")
        return False

    config['default_model'] = model_name
    if save_config(config):
        success_print(f"Default model set to '{color_text(model_name, CYAN)}'.")
        return True
    else:
        # save_config already printed an error
        return False


def remove_model_cmd(args: argparse.Namespace, config: Dict[str, Any]) -> bool:
    """Removes a configured model. Returns True on success."""
    model_name = args.model.strip()

    if not config.get('models'):
        error_print("No models configured yet.")
        return True # Nothing to remove, not really an error state

    if model_name not in config['models']:
        error_print(f"Model '{color_text(model_name, CYAN)}' not found in configuration.")
        list_models_cmd(args, config) # Show available models
        return False

    # Remove the model
    del config['models'][model_name]
    info_print(f"Removed model '{color_text(model_name, CYAN)}'.")

    # Check if it was the default model
    if config.get('default_model') == model_name:
        config['default_model'] = None
        warn_print(f"Removed model was the default. No default model is set now.")
        # Suggest setting a new default if other models exist
        if config['models']:
            info_print("You can set a new default using 'clippy set_default <model_name>'.")

    # Save the updated config
    if save_config(config):
        success_print(f"Configuration updated.")
        return True
    else:
        # save_config already printed an error
        return False


# --- Main Execution ---

def main() -> None:
    """Parses arguments and executes the corresponding command."""
    parser = argparse.ArgumentParser(
        description=f"{color_text('Clippy:', BOLD)} Your AI Command-Line Assistant.",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=f"Config: {CONFIG_FILE} | Repo: {CLIPPY_REPO_URL}",
        add_help=False # Disable default help to handle it manually if needed for default command
    )
    # Add explicit help option that works globally
    parser.add_argument(
        '-h', '--help', action='help', default=argparse.SUPPRESS,
        help='Show this help message and exit.'
    )

    subparsers = parser.add_subparsers(
        title='Commands',
        dest='command',
        # required=True, # Keep required=True initially for structure
        # We handle the default 'ask' command logic before parsing fully.
        # This means if no known command is found, we insert 'ask'.
        # If 'ask' is inserted, 'command' will be populated.
        # If only 'clippy' or 'clippy -h' is called, this logic is skipped or help is shown.
        help="Action to perform (if omitted, defaults to 'ask')",
        metavar="<command>" # Improve help display
    )

    # --- set_model command ---
    set_model_parser = subparsers.add_parser(
        'set_model',
        help='Configure an AI model: <model_name>:<api_key> or <provider>:<model>:<key>',
        description='Adds/updates model config. Provider (openai, google, anthropic) is inferred from model prefix (gpt-, gemini-, claude-) unless specified explicitly.',
        formatter_class=argparse.RawTextHelpFormatter # Allow newlines in description
    )
    set_model_parser.add_argument(
        'model_api',
        metavar='MODEL_CONFIG',
        help='Model string: e.g., "gpt-4o:sk-..." or "anthropic:claude-3-opus-20240229:sk-ant-..."'
    )
    set_model_parser.add_argument(
        '--default', '-d',
        action='store_true',
        help='Set this model as the default for `ask`.'
    )
    set_model_parser.set_defaults(func=set_model_cmd)

    # --- ask command ---
    # Make help text clearer that it's the default
    ask_parser = subparsers.add_parser(
        'ask',
        help='[Default] Ask the AI (reads prompt from args and stdin)',
        description='Sends prompt to the selected AI. Reads args first, then appends stdin if piped.',
        aliases=['a'], # Add short alias
        formatter_class=argparse.RawTextHelpFormatter
    )
    ask_parser.add_argument(
        'prompt',
        nargs='*', # 0 or more arguments for the prompt text
        metavar='PROMPT_TEXT',
        help='Text prompt for the AI. Reads from arguments and/or stdin pipe.'
    )
    ask_parser.add_argument(
        '--model', '-m',
        metavar='MODEL_NAME',
        help='Specify model to use (overrides default).'
    )
    # Example of adding more options to ask:
    # ask_parser.add_argument('--temp', type=float, default=0.7, help='Set query temperature (e.g., 0.5)')
    ask_parser.set_defaults(func=ask_cmd)

    # --- list command ---
    list_models_parser = subparsers.add_parser(
        'list',
        help='List configured models and the default',
        aliases=['ls'] # Add common alias
    )
    list_models_parser.set_defaults(func=list_models_cmd)

    # --- set_default command ---
    set_default_parser = subparsers.add_parser(
        'set_default',
        help='Set the default model for `ask`',
        aliases=['default']
    )
    set_default_parser.add_argument(
        'model',
        metavar='MODEL_NAME',
        help='Name of the configured model to set as default.'
    )
    set_default_parser.set_defaults(func=set_default_cmd)

    # --- remove_model command ---
    remove_model_parser = subparsers.add_parser(
        'remove_model',
        help='Remove a configured model',
        aliases=['rm'], # Add common alias
        description='Removes the specified model config. Unsets default if needed.'
    )
    remove_model_parser.add_argument(
        'model',
        metavar='MODEL_NAME',
        help='Name of the configured model to remove.'
    )
    remove_model_parser.set_defaults(func=remove_model_cmd)

    # --- Default Command Logic ---
    # Get all defined command names and aliases *before* parsing
    command_names_and_aliases = set()
    for name, choice_parser in subparsers.choices.items():
        command_names_and_aliases.add(name)
        if hasattr(choice_parser, 'aliases'):
            command_names_and_aliases.update(choice_parser.aliases)

    # Check sys.argv BEFORE parsing to see if a known command was provided
    args_to_parse = sys.argv[1:] # Arguments excluding the script name
    needs_ask_inserted = False

    if not args_to_parse:
        # No arguments given, `clippy` alone -> show help and exit
        parser.print_help()
        sys.exit(0)
    else:
        first_arg = args_to_parse[0]
        # If the first argument is NOT a known command AND NOT a help flag
        if first_arg not in command_names_and_aliases and first_arg not in ('-h', '--help'):
             # Assume it's a prompt for the 'ask' command
             needs_ask_inserted = True

    # If we determined it's a default command usage, modify args list for the parser
    if needs_ask_inserted:
        # Insert 'ask' command name at the beginning of the args list
        args_to_parse.insert(0, 'ask')
        # Now args_to_parse might be ['ask', 'my', 'prompt', 'here']
        # or ['ask', '-m', 'model_name', 'my', 'prompt']

    # Make subparsers required *after* potential insertion, so 'ask' satisfies it
    # Or, more simply, just leave required=True in the definition and the insertion handles it.
    # If `needs_ask_inserted` is False, the user provided a command, satisfying required=True.
    # If `needs_ask_inserted` is True, we inserted `ask`, satisfying required=True.
    # If no args were given, we exited above.
    # If only '-h' was given, argparse handles it via the global help action.

    # --- Check for updates (Run after arg check, before execution) ---
    try:
        # Only check updates if not explicitly asking for help
        if '-h' not in args_to_parse and '--help' not in args_to_parse:
            check_for_updates()
    except Exception as e:
        # Don't let the update check crash the main script
        warn_print(f"Update check failed with unexpected error: {e}")

    # --- Parse arguments ---
    # We pass the potentially modified list `args_to_parse`
    try:
        args = parser.parse_args(args_to_parse)
    except SystemExit as e:
         # Catch SystemExit raised by argparse on error (like missing required arg for a command)
         # Exit code is usually 2 for argparse errors
         sys.exit(e.code) # Propagate the exit code

    # --- Load config ---
    config = load_config()

    # --- Execute command ---
    if hasattr(args, 'func'):
         # Call the function associated with the parsed command
         try:
             success = args.func(args, config) # Pass config to command functions
             sys.exit(0 if success else 1) # Exit with 0 on success, 1 on handled failure
         except Exception as e:
             # Catch unexpected errors within command functions
             error_print(f"An unexpected error occurred during command execution: {e}")
             # import traceback # Optional: print traceback for debugging
             # traceback.print_exc(file=sys.stderr)
             sys.exit(2) # Use a different exit code for unexpected errors
    else:
        # This case should ideally not be reached if setup is correct
        # (either a command is found/inserted, or help is shown/exited)
        error_print("No command specified or recognized.")
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
