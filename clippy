#!/usr/bin/env python3

import argparse
import json
import os
import sys # Import the sys module
import urllib.request # Built-in HTTP client
import urllib.error # For handling HTTP errors
import urllib.parse # For URL encoding

# Configuration file location (in user's home directory)
CONFIG_DIR = os.path.expanduser("~/.clippy")
CONFIG_FILE = os.path.join(CONFIG_DIR, "config.json")

def load_config():
    """Loads configuration from the config file."""
    if not os.path.exists(CONFIG_FILE):
        return {}
    try:
        with open(CONFIG_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_config(config):
    """Saves configuration to the config file."""
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(CONFIG_FILE, 'w') as f:
        json.dump(config, f, indent=4)

def set_model(args):
    """Sets the OpenAI model and API key."""
    model_api_key = args.model_api.split(":", 1)
    if len(model_api_key) != 2:
        print("Error: Invalid model and API key format. Use <model_name>:<api_key>")
        return

    model_name, api_key = model_api_key
    config = load_config()
   # Initialize models dict if it doesn't exist
    if 'models' not in config:
        config['models'] = {}

    # Store the model configuration
    config['models'][model_name] = {
        'api_key': api_key
    }

    # Set as default if requested or if it's the only model
    if args.default or len(config['models']) == 1:
        config['default_model'] = model_name
        print(f"Model '{model_name}' set as default.")

    save_config(config)
    print(f"Model '{model_name}' configured successfully.")


def get_base_url(model_name):
    """
    Determines the base URL for the API based on the model name.
    """
    if model_name.startswith("gpt-"): # OpenAI models (e.g., gpt-4o, gpt-3.5-turbo)
        return "https://api.openai.com/v1/chat/completions"
    elif model_name.startswith("gemini-"): # Google models (using OpenAI compatible endpoint)
        return "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
    elif model_name.startswith("claude-"): # Anthropic models 
        return "https://api.anthropic.com/v1/messages"
    else:
        print(f"Warning: Unknown model prefix for '{model_name}'. Assuming OpenAI compatible API.")
        return "https://api.openai.com/v1/chat/completions"


class LightweightAIClient:
    """A lightweight client for making AI API requests using only built-in Python modules."""
    
    def __init__(self, api_key, base_url=None):
        self.api_key = api_key
        self.base_url = base_url or "https://api.openai.com/v1/chat/completions"
        
    def chat_completion(self, model, messages, temperature=0.7, max_tokens=None):
        """
        Makes a chat completion request to the AI API.
        
        Args:
            model (str): The model to use
            messages (list): List of message dictionaries with role and content
            temperature (float): Controls randomness (0 to 1)
            max_tokens (int, optional): Maximum tokens to generate
            
        Returns:
            dict: The API response
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
            
        # Handle Anthropic's different API format
        if "anthropic.com" in self.base_url:
            headers = {
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json"
            }
            
            system_content = None
            user_content = None
            
            for msg in messages:
                if msg["role"] == "system":
                    system_content = msg["content"]
                elif msg["role"] == "user":
                    user_content = msg["content"]
            
            payload = {
                "model": model,
                "max_tokens": max_tokens or 1000,
                "messages": [{"role": "user", "content": user_content}]
            }
            
            if system_content:
                payload["system"] = system_content
        
        # Convert the payload to JSON string
        data = json.dumps(payload).encode('utf-8')
        
        # Create a request object
        req = urllib.request.Request(
            url=self.base_url,
            data=data,
            headers=headers,
            method='POST'
        )
        
        try:
            # Make the request
            with urllib.request.urlopen(req) as response:
                # Read and decode the response
                response_body = response.read().decode('utf-8')
                return json.loads(response_body)
        except urllib.error.HTTPError as e:
            error_body = e.read().decode('utf-8')
            error_data = json.loads(error_body) if error_body else {"error": {"message": f"HTTP error: {e.code}"}}
            raise Exception(f"API request failed: {error_data.get('error', {}).get('message', str(e))}")
        except urllib.error.URLError as e:
            raise Exception(f"API connection failed: {str(e)}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")


def ask_ai(prompt, model_name, api_key, system_prompt=None):
    """
    Interacts with the AI API, supporting different providers.
    """
    print(f"Sending prompt to model '{model_name}'...")

    base_url = get_base_url(model_name)
    client = LightweightAIClient(api_key=api_key, base_url=base_url)

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    try:
        response = client.chat_completion(
            model=model_name,
            messages=messages
        )

        # Parse response based on API provider
        if "anthropic.com" in base_url:
            if "content" in response:
                return response["content"][0]["text"]
            else:
                return "No valid answer received from API."
        else:  # OpenAI-compatible format
            if "choices" in response and response["choices"]:
                ai_answer = response["choices"][0]["message"]["content"]
                return ai_answer
            else:
                return "No valid answer received from API."

    except Exception as e:
        print(f"An error occurred: {e}")
        return None


_SYSTEM_PROMPT = """
You are a helpful command-line assistant.  Please provide concise and well-formatted responses that are easy to read in a terminal.

Specifically:

- **Be brief and to the point.** Avoid unnecessary conversational fluff or lengthy explanations unless specifically asked for.
- **Format for readability in a terminal.** Use simple formatting like:
    - **Bold text** using markdown syntax `**bold text**` for emphasis where helpful.
    - **Lists** using markdown list syntax (`- item` or `1. item`) for itemized information.
    - **Code blocks** using markdown code block syntax (``` ```) to present code or commands clearly.
- **Keep lines reasonably short.**  Avoid extremely long lines that might wrap awkwardly in terminals with limited width.
- **Focus on providing the requested information directly.**  Assume the user is looking for quick, actionable answers.

Your goal is to be a helpful and efficient assistant that provides clear, concise, and well-formatted information suitable for a command-line environment.
"""

def _get_default_system_prompt():
    """Returns the default system prompt, augmented with OS information."""
    os_info = ""
    try:
        import platform
        os_info = f"  - **OS:** {platform.system()} {platform.release()} ({platform.machine()})"

    except ImportError:
        os_info = "  - **OS:** (Could not determine)"

    return _SYSTEM_PROMPT + "\n" + os_info


def ask(args):
    """Asks a question to the AI model."""
    config = load_config()

    if 'models' not in config or not config['models']:
        print("Error: No models configured. Please use 'clippy set_model <model_name>:<api_key>' first.")
        return

    # Use specified model or default model
    model_name = args.model or config.get('default_model')
    if not model_name:
        print("Error: No model specified and no default model set. Please specify a model with --model or set a default.")
        return

    if model_name not in config['models']:
        print(f"Error: Model '{model_name}' not found in configuration. Available models: {', '.join(config['models'].keys())}")
        return

    api_key = config['models'][model_name]['api_key']

    if not model_name or not api_key: # API key is now required
        print("Error: Model and API key not set. Please use 'clippy set_model <model_name>:<api_key>' first.")
        return

    prompt_parts = args.prompt
    stdin_content = ""

    if not sys.stdin.isatty(): # Check if stdin is not a terminal (i.e., data is being piped in)
        stdin_content = sys.stdin.read()
        if stdin_content:
            prompt_parts.append(stdin_content) # Append stdin content to the prompt

    prompt = " ".join(prompt_parts) # Join all parts of the prompt

    if not prompt.strip(): # Check if prompt is empty after potentially adding stdin
        print("Error: Please provide a prompt to ask, either as command line arguments or via stdin.")
        return

    ai_response = ask_ai(prompt, model_name, api_key,
                         system_prompt=_get_default_system_prompt())
    if ai_response:
        # Use ANSI escape codes for colored output
        RED = "\033[31m"
        GREEN = "\033[32m"
        YELLOW = "\033[33m"
        BLUE = "\033[34m"
        MAGENTA = "\033[35m"
        CYAN = "\033[36m"
        RESET = "\033[0m"

        print("\nAI Response:\n")
        
        # Highlight code blocks
        def highlight_code(text):
            lines = text.splitlines()
            in_code_block = False
            highlighted_lines = []
            for line in lines:
                if line.startswith("```"):
                    in_code_block = not in_code_block
                    highlighted_lines.append(line)  # Keep the backticks
                elif in_code_block:
                    highlighted_lines.append(CYAN + line + RESET)
                else:
                    highlighted_lines.append(line)
            return "\n".join(highlighted_lines)

        print(highlight_code(ai_response.strip())) # Remove leading/trailing whitespace for cleaner output


def list_models(args):
    """Lists all configured models and indicates the default model."""
    config = load_config()

    if 'models' not in config or not config['models']:
        print("No models configured. Use 'clippy set_model <model_name>:<api_key>' to add a model.")
        return

    default_model = config.get('default_model')
    print("\nConfigured Models:")
    for model_name in config['models']:
        prefix = "* " if model_name == default_model else "  "
        print(f"{prefix}{model_name}")

    if default_model:
        print("\n* indicates default model")

def change_default(args):
    """Changes the default model to another existing model."""
    config = load_config()

    if 'models' not in config or not config['models']:
        print("No models configured. Use 'clippy set_model <model_name>:<api_key>' first.")
        return

    model_name = args.model
    if model_name not in config['models']:
        print(f"Error: Model '{model_name}' not found in configuration.")
        print(f"Available models: {', '.join(config['models'].keys())}")
        return

    config['default_model'] = model_name
    save_config(config)
    print(f"Default model changed to '{model_name}'.")


def main():
    parser = argparse.ArgumentParser(description="Clippy: Your AI Command-Line Assistant (OpenAI, Google, Anthropic)")
    subparsers = parser.add_subparsers(title='commands', dest='command')

    # set_model command
    set_model_parser = subparsers.add_parser('set_model', help='Set an AI model and API key')
    set_model_parser.add_argument('model_api', help='Model name and API key in the format <model_name>:<api_key>')
    set_model_parser.add_argument('--default', '-d', action='store_true', help='Set this model as the default')
    set_model_parser.set_defaults(func=set_model)

    # ask command
    ask_parser = subparsers.add_parser('ask', help='Ask a question to the AI model')
    ask_parser.add_argument('prompt', nargs='+', help='The prompt to ask the AI')
    ask_parser.add_argument('--model', '-m', help='Specify which model to use (defaults to configured default model)')
    ask_parser.set_defaults(func=ask)

    # list_models command
    list_models_parser = subparsers.add_parser('list_models', help='List all configured models')
    list_models_parser.set_defaults(func=list_models)

    # change default model command
    change_default_parser = subparsers.add_parser('set_default', help='Change the default model')
    change_default_parser.add_argument('model', help='Name of the model to set as default')
    change_default_parser.set_defaults(func=change_default)

    args = parser.parse_args()

    if args.command:
        args.func(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
