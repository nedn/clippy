#!/usr/bin/env python3
"""
This script recursively combines the content of text files in a directory.
It uses a thread pool to read files in parallel, which can significantly speed up
the process for large directories.

The script supports optional file glob patterns to filter which files are processed.

The output is written to a file by default, but can be piped to stdout.
"""

import argparse
import sys
import os
import logging
from pathlib import Path
import concurrent.futures
import fnmatch  # For glob pattern matching on paths

try:
    import tiktoken
except ImportError:
    print("Warning: tiktoken is not installed, total token count will be disabled. Install it with 'pip install tiktoken'", file=sys.stderr)
    tiktoken = None

# --- Configuration ---
DEFAULT_OUTPUT_FILENAME = "output.txt"
MAX_WORKERS = os.cpu_count() or 4 # Use CPU count or default to 4 workers
READ_CHUNK_SIZE = 1024 * 1024 # Read in 1MB chunks for binary check

# --- Helper Functions ---

# Define a reasonable chunk size for reading
READ_CHUNK_SIZE = 1024  # Read 1KB chunk

# Common non-text file extensions (lowercase)
# This list is not exhaustive but covers many common binary formats.
NON_TEXT_EXTENSIONS = [
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.ico',
    '.heic', '.heif',

    # Audio
    '.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a', '.wma', '.aiff',

    # Video
    '.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.mpeg', '.mpg',

    # Compressed Archives
    '.zip', '.rar', '.tar', '.gz', '.bz2', '.7z', '.xz', '.iso', '.dmg',

    # Executables & Libraries
    '.exe', '.dll', '.so', '.dylib', '.app', '.msi',

    # Compiled Code / Object Files
    '.o', '.obj', '.class', '.pyc', '.pyo', '.wasm',

    # Documents (often binary or complex structure)
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.odt', '.ods', '.odp', # OpenDocument formats

    # Databases
    '.sqlite', '.db', '.mdb', '.accdb', '.dat', # .dat is ambiguous but often binary

    # Fonts
    '.ttf', '.otf', '.woff', '.woff2', '.eot',

    # Other common binary/non-text data
    '.bin', '.dat', # Reiterate .dat as it's common
    '.pickle', '.pkl', # Python serialized objects
    '.joblib',         # Scikit-learn models
    '.h5', '.hdf5',     # Hierarchical Data Format
    '.parquet',        # Columnar storage format
    '.avro',           # Data serialization system
    '.feather',        # Fast, lightweight file format
    '.arrow',          # Apache Arrow format
    '.model',          # Generic model files
    '.pt', '.pth',     # PyTorch models
    '.pb',             # Protocol Buffers (often used for ML models)
    '.onnx',           # Open Neural Network Exchange
    '.sav',            # SPSS data file
    '.dta',            # Stata data file
    '.idx',            # Often index files (binary)
    '.pack',           # Git pack files
    '.deb', '.rpm',    # Package manager files
    '.jar',            # Java archives
    '.war', '.ear',    # Java web/enterprise archives
    '.swf',            # Adobe Flash (obsolete but might be encountered)
    '.psd',            # Adobe Photoshop
    '.ai',             # Adobe Illustrator (often PDF compatible but proprietary)
    '.indd',           # Adobe InDesign
    '.blend',          # Blender 3D files
    '.dwg', '.dxf',    # CAD files (DXF can be text, but often complex)
    '.skp',            # SketchUp files
    '.stl',            # Stereolithography (3D printing)
    '.obj',            # Wavefront OBJ (can be text, but often large/complex geometry data)
    '.fbx',            # Autodesk FBX (3D models)
    '.gltf', '.glb',    # GL Transmission Format (glb is binary)
    '.swp',            # Vim swap file (binary)
    '.lock',           # Often empty or contain minimal binary data
]

# Convert to a set for slightly faster lookups, though for this size, a list is fine too.
NON_TEXT_EXTENSIONS_SET = set(NON_TEXT_EXTENSIONS)


def count_tokens(text: str) -> int:
    """Count the number of tokens in a text string."""
    if tiktoken is None:
        return -1 # Return -1 if tiktoken is not installed
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))


def is_likely_non_text(file_path: Path) -> bool:
    """
    Check if a file is likely non-text (binary).

    First checks the file extension against a list of known non-text types.
    If the extension doesn't match, it reads a small chunk of the file
    and checks for the presence of null bytes (a common indicator of binary data).

    Returns True if the file is likely non-text, False otherwise.
    Handles potential read errors gracefully.
    """
    # Check extension of file (case-insensitive)
    if file_path.suffix.lower() in NON_TEXT_EXTENSIONS_SET:
        return True

    # If extension isn't conclusive, check content for null bytes
    try:
        with file_path.open('rb') as f:
            chunk = f.read(READ_CHUNK_SIZE)
            # Check if a null byte exists in the chunk read
            # Empty files are considered not binary by this check
            return b'\0' in chunk
    except FileNotFoundError:
        print(f"Warning: File not found {file_path}", file=sys.stderr)
        return True # Treat as non-text if it disappeared or never existed
    except IsADirectoryError:
        print(f"Warning: Path is a directory, not a file {file_path}", file=sys.stderr)
        return True # Directories are not text files
    except PermissionError:
        print(f"Warning: Permission denied reading {file_path}", file=sys.stderr)
        return True # Treat as non-text if we can't read it
    except OSError as e:
        # Catch other potential OS-level errors during open/read
        print(f"Warning: Could not read {file_path} to check for binary content: {e}", file=sys.stderr)
        return True # Treat as non-text if we can't read it properly
    except Exception as e:
        # Catch any other unexpected errors
        print(f"Warning: Unexpected error checking binary status of {file_path}: {e}", file=sys.stderr)
        return True # Default to treating as non-text on unexpected errors
    

def should_ignore(file_path: Path, root_dir: Path, pattern: str) -> bool:
    """
    Check if a file should be ignored based on defined rules:
    - Hidden file (starts with '.')
    - Inside a hidden directory (any parent part starts with '.')
    - Does not match the glob pattern.
    - Is likely binary.
    """
    # 1. Check if it's actually a file (resolve symlinks first)
    try:
        if not file_path.is_file():
            # This might happen with broken symlinks during the walk
            return True
    except OSError as e:
        # Could be a permission error or other issue accessing the file type
        print(f"Warning: Could not determine if {file_path} is a file: {e}", file=sys.stderr)
        return True # Ignore if we can't verify it's a file

    # Use relative path for hidden checks and pattern matching
    try:
        relative_path = file_path.relative_to(root_dir)
        relative_path_str = str(relative_path)
    except ValueError:
        # Should not happen if file_path is within root_dir, but handle defensively
         print(f"Warning: Could not get relative path for {file_path} based on {root_dir}", file=sys.stderr)
         return True


    # 2. Check glob pattern first (often the most selective)
    if pattern != '*' and not fnmatch.fnmatch(relative_path_str, pattern) and not fnmatch.fnmatch(file_path.name, pattern):
         # Check against both relative path and just the filename
        return True

    # 3. Check for hidden file/directory
    # Check filename itself
    if file_path.name.startswith('.'):
        return True
    # Check any parent directory component
    # Use relative_path.parts to avoid checking parts outside the root_dir
    if any(part.startswith('.') for part in relative_path.parts[:-1]): # Check parent parts
         return True

    # 4. Check for binary content (can be slow, do last)
    if is_likely_non_text(file_path):
        logging.info("Skipping likely non text file: {relative_path_str}", file=sys.stderr)
        return True

    return False # If none of the ignore conditions match

def read_file_content(file_path: Path, root_dir: Path) -> tuple[str, str] | None:
    """
    Reads the content of a text file.
    Returns a tuple (relative_path_str, content) or None if reading fails.
    """
    try:
        relative_path = file_path.relative_to(root_dir)
        relative_path_str = str(relative_path)
        with file_path.open('r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        return (relative_path_str, content)
    except OSError as e:
        print(f"Warning: Could not read file {file_path}: {e}", file=sys.stderr)
        return None
    except UnicodeDecodeError as e:
        # Should ideally be caught by is_likely_binary, but as a fallback
        print(f"Warning: Skipping file with encoding issues {file_path}: {e}", file=sys.stderr)
        return None
    except Exception as e:
         print(f"Warning: Unexpected error reading file {file_path}: {e}", file=sys.stderr)
         return None

# --- Main Logic ---

def main():
    parser = argparse.ArgumentParser(
        description="Recursively combine content of text files in a directory.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "directory",
        nargs="?",
        default=".",
        help="The directory to scan recursively. Defaults to the current directory."
    )
    parser.add_argument(
        "-p", "--pattern",
        default="*",
        help="Optional file glob pattern (e.g., '*.py', 'src/**/test_*.py'). "
             "Filters files based on their relative path within the target directory."
    )
    parser.add_argument(
        "-w", "--workers",
        type=int,
        default=MAX_WORKERS,
        help="Number of parallel workers for reading files."
    )

    args = parser.parse_args()

    root_dir = Path(args.directory).resolve() # Get absolute path
    file_pattern = args.pattern
    num_workers = args.workers

    if not root_dir.is_dir():
        print(f"Error: Directory not found: {args.directory}", file=sys.stderr)
        sys.exit(1)

    print(f"Scanning directory: {root_dir}", file=sys.stderr)
    print(f"Using file pattern: {file_pattern}", file=sys.stderr)
    print(f"Ignoring hidden files/directories and binary files.", file=sys.stderr)

    # --- File Discovery ---
    # Use rglob which handles recursion naturally.
    # We collect all potential paths first.
    all_files = []
    try:
        # Using generator directly is fine here as filtering happens next
        file_iterator = root_dir.rglob('*')
        all_files = list(file_iterator)
    except PermissionError as e:
         print(f"Warning: Permission denied during directory scan: {e}", file=sys.stderr)
         # Continue with files found so far
    except Exception as e:
         print(f"Error during directory scan: {e}", file=sys.stderr)
         sys.exit(1)


    # --- Filtering ---
    print(f"Found {len(all_files)} potential items. Filtering...", file=sys.stderr)
    files_to_process = []
    for file_path in all_files:
        if not should_ignore(file_path, root_dir, file_pattern):
            files_to_process.append(file_path)

    # Sort files for consistent output order
    files_to_process.sort()

    print(f"Processing {len(files_to_process)} files...", file=sys.stderr)

    # --- Parallel Reading ---
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Submit tasks
        future_to_path = {
            executor.submit(read_file_content, path, root_dir): path
            for path in files_to_process
        }

        # Collect results as they complete
        processed_count = 0
        for future in concurrent.futures.as_completed(future_to_path):
            path = future_to_path[future]
            try:
                result = future.result()
                if result:
                    results.append(result)
                processed_count += 1
                # Optional: Progress indicator
                print(f"\rProcessed: {processed_count}/{len(files_to_process)}", end="", file=sys.stderr)

            except Exception as e:
                print(f"\nError processing file {path}: {e}", file=sys.stderr)

    print("\nReading complete.", file=sys.stderr) # Newline after progress indicator

    # --- Sort results by relative path (important as futures may complete out of order) ---
    results.sort(key=lambda item: item[0]) # Sort by relative_path_str

    # --- Determine Output Destination ---
    output_target = None
    using_stdout = False
    if sys.stdout.isatty():
        # Output is to a terminal, write to default file
        output_filename = DEFAULT_OUTPUT_FILENAME
        print(f"Outputting to file: {os.path.abspath(output_filename)}", file=sys.stderr)
        try:
            output_target = open(output_filename, 'w', encoding='utf-8')
        except OSError as e:
            print(f"Error: Could not open output file {output_filename}: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Output is piped or redirected, write to stdout
        print(f"Outputting to stdout", file=sys.stderr)
        output_target = sys.stdout
        using_stdout = True

    # --- Writing Output ---
    total_tokens = 0
    try:
        file_count = 0
        for relative_path, content in results:
            file_info = f">>>> {relative_path}\n"
            output_target.write(file_info)
            output_target.write(content)
            total_tokens += count_tokens(file_info) + count_tokens(content)
            # Add a newline if content doesn't end with one for better separation
            if content and not content.endswith('\n'):
                 output_target.write('\n')
            output_target.flush() # Flush periodically for long outputs
            file_count += 1
    except Exception as e:
         print(f"\nError writing output: {e}", file=sys.stderr)
         # Avoid traceback flood if stdout pipe is broken
         if isinstance(e, BrokenPipeError):
             sys.exit(0) # Exit cleanly if pipe is broken
         else:
             sys.exit(1)
    finally:
        if output_target and not using_stdout:
            output_target.close()

    print(f"\nSuccessfully combined content of {file_count} files.", file=sys.stderr)
    if total_tokens > 0:
        print(f"Total tokens (approximate): {total_tokens:,}", file=sys.stderr)

if __name__ == "__main__":
    main()
