#!/usr/bin/env python3

import argparse
import sys
import os
from pathlib import Path
import concurrent.futures
import fnmatch  # For glob pattern matching on paths

# --- Configuration ---
DEFAULT_OUTPUT_FILENAME = "output.txt"
MAX_WORKERS = os.cpu_count() or 4 # Use CPU count or default to 4 workers
READ_CHUNK_SIZE = 1024 * 1024 # Read in 1MB chunks for binary check

# --- Helper Functions ---

def is_likely_binary(file_path: Path) -> bool:
    """
    Check if a file is likely binary.
    Reads a chunk and checks for null bytes. More robust checks exist,
    but this is a common and often sufficient heuristic.
    Returns True if likely binary, False otherwise.
    Handles potential read errors.
    """
    try:
        with file_path.open('rb') as f:
            chunk = f.read(READ_CHUNK_SIZE)
            return b'\0' in chunk
    except OSError as e:
        print(f"Warning: Could not read {file_path} to check for binary content: {e}", file=sys.stderr)
        return True # Treat as binary if we can't read it properly
    except Exception as e:
        print(f"Warning: Unexpected error checking binary status of {file_path}: {e}", file=sys.stderr)
        return True

def should_ignore(file_path: Path, root_dir: Path, pattern: str) -> bool:
    """
    Check if a file should be ignored based on defined rules:
    - Hidden file (starts with '.')
    - Inside a hidden directory (any parent part starts with '.')
    - Does not match the glob pattern.
    - Is likely binary.
    """
    # 1. Check if it's actually a file (resolve symlinks first)
    try:
        if not file_path.is_file():
            # This might happen with broken symlinks during the walk
            return True
    except OSError as e:
        # Could be a permission error or other issue accessing the file type
        print(f"Warning: Could not determine if {file_path} is a file: {e}", file=sys.stderr)
        return True # Ignore if we can't verify it's a file

    # Use relative path for hidden checks and pattern matching
    try:
        relative_path = file_path.relative_to(root_dir)
        relative_path_str = str(relative_path)
    except ValueError:
        # Should not happen if file_path is within root_dir, but handle defensively
         print(f"Warning: Could not get relative path for {file_path} based on {root_dir}", file=sys.stderr)
         return True


    # 2. Check glob pattern first (often the most selective)
    if pattern != '*' and not fnmatch.fnmatch(relative_path_str, pattern) and not fnmatch.fnmatch(file_path.name, pattern):
         # Check against both relative path and just the filename
        return True

    # 3. Check for hidden file/directory
    # Check filename itself
    if file_path.name.startswith('.'):
        return True
    # Check any parent directory component
    # Use relative_path.parts to avoid checking parts outside the root_dir
    if any(part.startswith('.') for part in relative_path.parts[:-1]): # Check parent parts
         return True

    # 4. Check for binary content (can be slow, do last)
    if is_likely_binary(file_path):
        print(f"Info: Skipping likely binary file: {relative_path_str}", file=sys.stderr)
        return True

    return False # If none of the ignore conditions match

def read_file_content(file_path: Path, root_dir: Path) -> tuple[str, str] | None:
    """
    Reads the content of a text file.
    Returns a tuple (relative_path_str, content) or None if reading fails.
    """
    try:
        relative_path = file_path.relative_to(root_dir)
        relative_path_str = str(relative_path)
        with file_path.open('r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        return (relative_path_str, content)
    except OSError as e:
        print(f"Warning: Could not read file {file_path}: {e}", file=sys.stderr)
        return None
    except UnicodeDecodeError as e:
        # Should ideally be caught by is_likely_binary, but as a fallback
        print(f"Warning: Skipping file with encoding issues {file_path}: {e}", file=sys.stderr)
        return None
    except Exception as e:
         print(f"Warning: Unexpected error reading file {file_path}: {e}", file=sys.stderr)
         return None

# --- Main Logic ---

def main():
    parser = argparse.ArgumentParser(
        description="Recursively combine content of text files in a directory.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "directory",
        nargs="?",
        default=".",
        help="The directory to scan recursively. Defaults to the current directory."
    )
    parser.add_argument(
        "-p", "--pattern",
        default="*",
        help="Optional file glob pattern (e.g., '*.py', 'src/**/test_*.py'). "
             "Filters files based on their relative path within the target directory."
    )
    parser.add_argument(
        "-w", "--workers",
        type=int,
        default=MAX_WORKERS,
        help="Number of parallel workers for reading files."
    )

    args = parser.parse_args()

    root_dir = Path(args.directory).resolve() # Get absolute path
    file_pattern = args.pattern
    num_workers = args.workers

    if not root_dir.is_dir():
        print(f"Error: Directory not found: {args.directory}", file=sys.stderr)
        sys.exit(1)

    print(f"Scanning directory: {root_dir}", file=sys.stderr)
    print(f"Using file pattern: {file_pattern}", file=sys.stderr)
    print(f"Ignoring hidden files/directories and binary files.", file=sys.stderr)

    # --- File Discovery ---
    # Use rglob which handles recursion naturally.
    # We collect all potential paths first.
    all_files = []
    try:
        # Using generator directly is fine here as filtering happens next
        file_iterator = root_dir.rglob('*')
        all_files = list(file_iterator)
    except PermissionError as e:
         print(f"Warning: Permission denied during directory scan: {e}", file=sys.stderr)
         # Continue with files found so far
    except Exception as e:
         print(f"Error during directory scan: {e}", file=sys.stderr)
         sys.exit(1)


    # --- Filtering ---
    print(f"Found {len(all_files)} potential items. Filtering...", file=sys.stderr)
    files_to_process = []
    for file_path in all_files:
        if not should_ignore(file_path, root_dir, file_pattern):
            files_to_process.append(file_path)

    # Sort files for consistent output order
    files_to_process.sort()

    print(f"Processing {len(files_to_process)} files...", file=sys.stderr)

    # --- Parallel Reading ---
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Submit tasks
        future_to_path = {
            executor.submit(read_file_content, path, root_dir): path
            for path in files_to_process
        }

        # Collect results as they complete
        processed_count = 0
        for future in concurrent.futures.as_completed(future_to_path):
            path = future_to_path[future]
            try:
                result = future.result()
                if result:
                    results.append(result)
                processed_count += 1
                # Optional: Progress indicator
                print(f"\rProcessed: {processed_count}/{len(files_to_process)}", end="", file=sys.stderr)

            except Exception as e:
                print(f"\nError processing file {path}: {e}", file=sys.stderr)

    print("\nReading complete.", file=sys.stderr) # Newline after progress indicator

    # --- Sort results by relative path (important as futures may complete out of order) ---
    results.sort(key=lambda item: item[0]) # Sort by relative_path_str

    # --- Determine Output Destination ---
    output_target = None
    using_stdout = False
    if sys.stdout.isatty():
        # Output is to a terminal, write to default file
        output_filename = DEFAULT_OUTPUT_FILENAME
        print(f"Outputting to file: {output_filename}", file=sys.stderr)
        try:
            output_target = open(output_filename, 'w', encoding='utf-8')
        except OSError as e:
            print(f"Error: Could not open output file {output_filename}: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Output is piped or redirected, write to stdout
        print(f"Outputting to stdout", file=sys.stderr)
        output_target = sys.stdout
        using_stdout = True

    # --- Writing Output ---
    try:
        file_count = 0
        for relative_path, content in results:
            output_target.write(f">>>> {relative_path}\n")
            output_target.write(content)
            # Add a newline if content doesn't end with one for better separation
            if content and not content.endswith('\n'):
                 output_target.write('\n')
            output_target.flush() # Flush periodically for long outputs
            file_count += 1
    except Exception as e:
         print(f"\nError writing output: {e}", file=sys.stderr)
         # Avoid traceback flood if stdout pipe is broken
         if isinstance(e, BrokenPipeError):
             sys.exit(0) # Exit cleanly if pipe is broken
         else:
             sys.exit(1)
    finally:
        if output_target and not using_stdout:
            output_target.close()

    print(f"\nSuccessfully combined content of {file_count} files.", file=sys.stderr)

if __name__ == "__main__":
    main()
